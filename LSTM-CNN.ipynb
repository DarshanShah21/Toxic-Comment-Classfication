{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory - Covolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a combination of LSTM-CNN. LSTM is implemented with GRU layer in Keras and 1D convolution layer is built on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense, Input, Bidirectional, Conv1D, GRU\n",
    "from keras.layers import SpatialDropout1D, Embedding, concatenate\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 150\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    This function loads text data from csv files, converts into numeric\n",
    "    values  and retunrs as train, test and labels. It also returns embedding\n",
    "    matrix which stores weights of embedding layer for each word.\n",
    "    '''\n",
    "    print('Loading data...')\n",
    "    # If your data is stored somewhere else, give \n",
    "    # appropriate path here\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "    train[\"comment_text\"].fillna(\"fillna\")\n",
    "    test[\"comment_text\"].fillna(\"fillna\")\n",
    "    X_train = train[\"comment_text\"].str.lower()\n",
    "    X_test = test[\"comment_text\"].str.lower()\n",
    "    y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "    \n",
    "    # Convert text to numeric sequences and pad them\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    \n",
    "    data = (x_train, y_train, x_test)\n",
    "        \n",
    "    # Get weights for embedding layer\n",
    "    embedding_matrix = get_embedding_weights(tokenizer)\n",
    "    \n",
    "    return data, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create embedding matrix\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights for the embedding layer from a pre-trained model\n",
    "def get_embedding_weights(tokenizer):\n",
    "    \n",
    "    print('Getting weights for the embedding layer...')\n",
    "    \n",
    "    # Create a dictionary to store mappings of pre-trained \n",
    "    # embedding weights from the embedding file\n",
    "    # Give appropriate path here\n",
    "    EMBEDDING_FILE = './crawl-300d-2M.vec'\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    # Get the dictionary of word to index mappings for our data\n",
    "    word_index = tokenizer.word_index\n",
    "    # Count number of words from it\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    \n",
    "    # Create an embedding matrix of appropriate size\n",
    "    # to store weights for the words in our data\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    \n",
    "    # Iterate through each word in the data to get its\n",
    "    # corresponding weight\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(embedding_matrix):\n",
    "    \n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, \n",
    "                  embed_size, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(data, model, batch_size=128, epochs=3):\n",
    "    \n",
    "    x_train, y_train, x_test = data\n",
    "\n",
    "    print('Splitting data...')\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        x_train, y_train, train_size=0.90, random_state=233)\n",
    "    \n",
    "    X_train.reshape((X_train.shape[0], maxlen))\n",
    "    X_val.reshape((X_val.shape[0], maxlen))\n",
    "\n",
    "\n",
    "    print('Training the model...')\n",
    "    # Fir the data\n",
    "    hist = model.fit(X_train, y_train, \n",
    "                     batch_size=batch_size, epochs=epochs, \n",
    "                     validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "    print('Predicting on the unseen data...')\n",
    "    # Predict\n",
    "    y_pred = model.predict(x_test, batch_size=1024)\n",
    "    \n",
    "    # Save results\n",
    "    submission = pd.read_csv('./data/sample_submission.csv')\n",
    "    submission[\n",
    "        ['toxic', 'severe_toxic', 'obscene', \n",
    "         'threat', 'insult', 'identity_hate']] = y_pred\n",
    "    submission.to_csv('submission_lstm-cnn.csv', index=False)\n",
    "    \n",
    "    print('Predictions done... Results saved successfully!')\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "data, embedding_matrix = load_data()\n",
    "\n",
    "# Compile model\n",
    "model = compile_model(embedding_matrix)\n",
    "\n",
    "# Run network\n",
    "hist = run_network(data, model)\n",
    "\n",
    "with open('./lstm-cnn_history.json', 'w') as f:\n",
    "    json.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
